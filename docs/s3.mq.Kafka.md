## Tag s3.mq.Kafka
Example usage of message queue in obo-payment.

### Overview
Payment service is generic/supporting subdomain of Online Box Office.
It is used to finish the payment of a tickets order. It's not the core domain, it just invokes some public service.
But the payment information can be used for the cinema to analysis financial situation.
See [here](../obo-payment/README.md) for its business flow.

Almost all the third party payment service is not synchronous.
They will invoke a callback function or API to notify the payment result.
So obo-payment should be asynchronous, the order should be marked as 'paying'. Once obo-payment gets the payment result,
it should publish an event asynchronously. obo-payment can also call obo-trade REST API, but obo-payment should not depend on obo-trade.

Furthermore, payment completion may trigger other actions which are not clear at this moment. By publishing event, the dependencies are decoupled.
We can easily add new actions in the future.

In DDD, a domain event is a full-fledged par of the domain model, a representation of something that happened in the domain.	
And message queue like RabbitMQ, Kafka is recommended to be used to publish events.

### MQ
MQ(message queue) is typically used for inter-process communication (IPC), or for inter-thread communication within the same process.
* Asynchronous communication
* FIFO

Some general use cases of message queue includes:
* Asynchronous invocation vs. RPC
* Decoupling
* Peak shaving and valley filling

Distributed transaction can be seen as one of the use cases for asynchronous invocation. We implemented this feature in the current tag.
Comparison between some popular MQ:

| Name                                    | Start | Originator               | Current          | Language    | Destination | Basic Ideology                                                                  |
|-----------------------------------------|-------|--------------------------|------------------|-------------|-------------|---------------------------------------------------------------------------------|
| RabbitMQ                                | 2007  | Rabbit Technologies Ltd. | VMware ->Pivotal | Erlang      | Exchange    | Queue based, subscription is also based on Queue<br/>Supports AMQP, STOMP, MQIT |
| Kafka                                   | 2011  | Linkin                   | Apache           | Java,Scalar | Topic       | Topic based, queue is implemented based on consumer group                       |
| NATS(Neural Autonomic Transport System) | 2011  | Derek Collison           | CNCF             | Go          | Subject     | Core NATS vs. JetStream                                                         |

### Kafka
[Apache Kafka](https://kafka.apache.org/) is an open-source distributed event store and streaming platform used for high-performance data pipelines, stream analytics, data integration and so on.
1. [Download](https://kafka.apache.org/downloads) and extract Kafka
2. Kafka can be started using ZooKeeper or KRaft. To start with ZooKeeper:
* Start the ZooKeeper service
```shell
# Linux
$ bin/zookeeper-server-start.sh config/zookeeper.properties
# Windows
D:\Tools\kafka_2.13-3.2.1> bin\windows\zookeeper-server-start.bat config\zookeeper.properties
```

* Open another terminal session and run:
```shell
# Linux
$ bin/kafka-server-start.sh config/server.properties
# Windows
D:\Tools\kafka_2.13-3.2.1> bin\windows\kafka-server-start.bat config\server.properties
```
3. Create topic for obo-payment
```shell
# Linux
$  bin/kafka-topics.bat --create --topic payment-result --bootstrap-server localhost:9092
# Windows
D:\Tools\kafka_2.13-3.2.1>bin\windows\kafka-topics.bat --create --topic payment-result --bootstrap-server localhost:9092
```
4. Try publishing/subscribing events to 'payment-result'
```shell
# Linux
$ bin/kafka-console-producer.sh --topic payment-result --bootstrap-server localhost:9092
$ bin/kafka-console-consumer.sh --topic payment-result --from-beginning --bootstrap-server localhost:9092
# Windows
D:\Tools\kafka_2.13-3.2.1> bin\windows\kafka-console-producer.bat --topic payment-result --bootstrap-server localhost:9092
D:\Tools\kafka_2.13-3.2.1> bin\windows\kafka-console-consumer.bat --topic payment-result --from-beginning --bootstrap-server localhost:9092
```

See [here](https://kafka.apache.org/quickstart) for more information.

### Spring Cloud Stream
[Spring Cloud Stream](https://spring.io/projects/spring-cloud-stream) is a framework for building highly scalable event-driven microservices connected with shared messaging systems.
It officially supports RabbitMQ, Kafka. For RocketMQ, please refer to [Spring Cloud Alibaba RocketMQ Binder](https://spring-cloud-alibaba-group.github.io/github-pages/2021/en-us/index.html#_spring_cloud_alibaba_rocketmq_binder).
And for NATS, please refer to [nats-spring-cloud-stream-binder](https://github.com/nats-io/spring-nats/tree/main/nats-spring-cloud-stream-binder).

Spring Cloud Stream provides a number of abstractions and primitives that simplify the writing of message-driven microservice applications.
The most important one is **_binding_** abstraction. The application communicates with the outside world by establishing bindings between destinations exposed by the external brokers and input/output arguments in your code. 
Broker specific details necessary to establish bindings are handled by middleware-specific binder implementations.
For example, if we want to use Kafka as a MQ middleware, we need add its binder implementation:

1. Add Kafka starter of Spring Cloud Stream binder
```xml
        <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-stream-kafka</artifactId>
        </dependency>
```
Then set up the **_binding_** by configuration:
2. Configuration for Kafka binder and binding
* Binders: Components responsible to provide integration with the external messaging systems.
* Bindings: Bridge between the external messaging systems and application provided Producers and Consumers of messages (created by the Destination Binders).
```yaml
spring:
  cloud:
    stream:
      kafka:
        binder:
          brokers: localhost
          default-broker-port: 9092
      bindings:
        payment-out-0:
          destination: payment-result
          use-native-encoding: true
```
We created a Kafka binder to broker of 'localhost:9092', and a binding named 'payment-out-0'.
'payment-out-0' is a logical name, its real destination is 'payment-result'.  

3. StreamBridge

StreamBridge is a mechanism in Spring Cloud Stream to send arbitrary data to an output.
```java
    @Autowired
    private StreamBridge bridge;

    public String changeStatus(String paymentId, String status) {
        if (!status.equals(Payment.Status.SUCCESSFUL.name()) && !status.equals(Payment.Status.FAILED.name())) {
        throw new IllegalStateException("Status can only be SUCCESSFUL or FAILED, instead of "+status);
        }
        Optional<Payment> optional = paymentRepository.findById(paymentId);
        if (!optional.isPresent()) {
        return null;
        }
        Payment payment = optional.get();
        payment.setStatus(Payment.Status.valueOf(status));
        payment.setFinishedTime(LocalDateTime.now());

        bridge.send("payment-result", payment.getOrderId()+":"+status);
        // bridge.send("payment-out-0", payment.getOrderId()+":"+status);

        return payment.getId();
    }
```
The first argument of StreamBridge,send method is the destination name. Both binding name and real topic name can be used here.

### Spring Cloud Function support

The binding name 'payment-out-0' looks strange, but it follows the naming convention to support functional programming.
java.util.function.[Supplier/Function/Consumer] were well-supported since Spring Cloud Stream v2.1, 
where Supplier is the source of events, Consumer is the handler of events, and Function can be both source and handler of events.
Under this mechanism, functions injected will be used to process event automatically.

In our case, we injected a Consumer in obo-trade to as the handler of event, and configured bindings according to the naming convention.

1. create consumer of message queue
```java
@Configuration
@Slf4j
public class EventBindings {
    @Autowired
    private OrderService orderService;

    @Bean
    public Consumer<String> payment() {
        return result -> {
            log.debug("received a message:{}", result);
            String[] results = result.split(":");
            String orderId = results[0];
            String status = results[1];
            if (status.equals("SUCCESSFUL")) {
                status = "PAID";
            } else {
                status = "FAILED";
            }
            try {
                log.info("Update order({}) to status {}", orderId, status);
                orderService.finishOrder(orderId, status);
            } catch (Throwable e) {
                log.error("Got exception: {}", e.getMessage());
            }
        };
    }
}
```
In the preceding code example, a Consumer 'payment' is injected to Spring. According to the naming convention, it will be trigger when there are events in 'payment-in-0'.

2. configure binding 'payment-in-0'
```yaml
spring:
  cloud:
    stream:
      bindings:
        payment-in-0:
          group: obo-trade-payment
          destination: payment-result
          use-native-encoding: true
      kafka:
        binder:
          brokers: localhost
          default-broker-port: 9092
        bindings:
          payment-in-0:
            consumer:
              startOffset: latest
```
The naming convention used to name input and output bindings is as follows:
* input:  &lt;functionName&gt; + -in- + &lt;index&gt;
* output: &lt;functionName&gt; + -out- + &lt;index&gt;

The in and out corresponds to the type of binding (such as input or output). The index is the index of the input or output binding. 
It is always 0 for typical single input/output function, so itâ€™s only relevant for Functions with multiple input and output arguments.

For java.util.function.Function, it receives messages, processed them, and send the result to another queue.
For example, if we have a Function named 'test', it will receive messages from 'test-in-0', and send the result to 'test-out-0'.

java.util.function.Supplier is a little different, it is not triggered by event. It's invoked by the framework periodically, one invocation each second by default.
This can be changed by configuration with prefix `spring.cloud.stream.poller`:
* fixedDelay(ms): default poller in milliseconds, default 1000
* maxMessagesPerPoll: Maximum messages for each polling event of the default poller, default 1
* cron: Cron expression value for the Cron Trigger
* initialDelay: Initial delay for periodic triggers
* timeUnit: default MILLISECONDS

3. consumer group

Spring Cloud Stream consumer groups are similar to and inspired by Kafka consumer groups.
All groups that subscribe to a given destination receive a copy of published data, but only one member of each group receives a given message from that destination. 
By default, when a group is not specified, Spring Cloud Stream assigns the application to an anonymous and independent single-member consumer group that is in a publish-subscribe relationship with all other consumer groups.
Each consumer binding can use the `spring.cloud.stream.bindings.<bindingName>.group` property to specify a group name.

**_Consumer Group is very important in microservices. It can also be used to implement distributed scheduler/timer._**

## Try yourself
Follow the instructions to finish the tasks
### Kafka Start
1. open Kafka installation directory
2. change Zookeeper configuration "**_config\zookeeper.properties_**": `dataDir=D:/tmp/zookeeper`
3. start Zookeeper: `bin\windows\zookeeper-server-start.bat config\zookeeper.properties`
4. change Kafka configuration "**_config\server.peroperties_**": `log.dirs=D:/tmp/kafka-logs`
5. start Kafka Server: `bin\windows\kafka-server-start.bat config\server.properties`
### Kafka Topic
1. open kafka log dir: `D:/tmp/kafka-logs`, check what's in it
2. list topics: `bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list`
3. create topics: `bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic test`
4. list topics by preceding command
5. check what's changed in Kafka log dir
6. stop kafka, change configuration: `num.partitions=2`, create another topic 'test1', see what happened
```
Notes: Don't delete topic on Windows, it will report access denied exception
```
### Console Producer and Consumer
1. start console producer: `bin\windows\kafka-console-producer.bat --bootstrap-server localhost:9092 --topic test`
2. start console consumer: `bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test`
3. send messages in producer shell window, see the logs in consumer shell window
### Finish an Order
1. start mysqld, obo-eureka, obo-seata
2. start obo-schedule, obo-trade, obo-payment
3. run `bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list` to check the new topic
4. create an order, POST http://localhost:8085/obo/order, request body as following:
```json
{
  "phone": "13999999999",
  "orderItemList": [
    {
      "ticket": {
        "ticketId": "test-ticket-id",
        "seatFloor": 1,
        "seatRow": 1,
        "seatCol": 1
      },
      "price": 40
    }
  ]
}
```
5. copy the returned order id 
6. pay the order, PUT http://localhost:8085/obo/order/90859e54-9953-49ec-a6fe-898505abf8a4/payment?fromAccount=1&toAccount=2
7. copy the payment order id
8. invoke payment callback, PUT http://localhost:8083/obo/payment/f9dcd450-08ce-43b0-8057-97b434880a53/status/SUCCESSFUL
9. check the source code in obo-payment and obo-trade related to messaging
10. login mysql to check the status change